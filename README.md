ğŸ” Adversarial Attack Simulator for AI Models

This project is a GUI-based adversarial attack simulation tool designed to test the robustness and security of machine learning models against common adversarial attacks. It allows users to visually analyze how small, malicious perturbations can mislead AI models.

The simulator is built using Python, PyTorch, and a graphical user interface (GUI), making it beginner-friendly while still being powerful enough for AI security experimentation.

ğŸš€ Features

ğŸ“Š Interactive GUI interface

ğŸ§  Pretrained CNN model (MNIST)

âš”ï¸ Supports multiple adversarial attacks:

FGSM (Fast Gradient Sign Method)

PGD (Projected Gradient Descent)

Data Poisoning Attack

Evasion Attack

ğŸ–¼ï¸ Visual comparison of:

Original image

Adversarial image

ğŸ“‰ Model predictions before and after attack

ğŸ›ï¸ Adjustable attack parameters (epsilon, steps, learning rate)

ğŸ“¦ Modular & scalable project structure

ğŸ› ï¸ Tech Stack

Python

PyTorch & Torchvision

Tkinter / PyQt5

NumPy

Matplotlib

Pillow

ğŸ§ª Attacks Implemented
ğŸ”¹ FGSM (Fast Gradient Sign Method)

Generates adversarial examples using a single-step gradient-based perturbation.

ğŸ”¹ PGD (Projected Gradient Descent)

An iterative version of FGSM that creates stronger adversarial attacks.

ğŸ”¹ Data Poisoning

Introduces malicious samples into training data to compromise model learning.

ğŸ”¹ Evasion Attack

Modifies inputs at inference time to evade correct classification.

ğŸ–¥ï¸ GUI Preview

The application provides a user-friendly interface to:

Select attack type

Load models

Tune parameters

Run attacks

Visualize results

(Screenshots can be added here)

ğŸ“‚ Project Structure
adversarial_attack_simulator/
â”œâ”€â”€ gui/
â”œâ”€â”€ attacks/
â”œâ”€â”€ models/
â”œâ”€â”€ utils/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš ï¸ Ethical Disclaimer

This project is intended strictly for educational and research purposes.
It must not be used to harm systems, violate privacy, or exploit real-world AI deployments.

ğŸ¯ Use Cases

AI Security Research

Adversarial Machine Learning Learning

College / Final Year Project

ML Robustness Testing

Resume & Portfolio Project

ğŸ“Œ Future Enhancements

Support for CIFAR-10 and custom datasets

Defense mechanisms (Adversarial Training)

Model robustness metrics

Report export (PDF)

Web-based version

ğŸ¤ Contributions

Contributions, issues, and feature requests are welcome!
Feel free to fork the repository and submit a pull request.

â­ If You Like This Project

Give it a â­ on GitHub to support the project!
